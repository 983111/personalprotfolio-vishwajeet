<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vishwajeet Adkine — Research CV</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,500;0,600;1,400&family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --bg: #fafafa;
            --card: #ffffff;
            --ink: #111827;
            --muted: #6b7280;
            --accent: #1d4ed8;
            --accent-light: #eff6ff;
            --border: #e5e7eb;
            --rule: #d1d5db;
            --green: #065f46;
            --green-bg: #ecfdf5;
        }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Inter', sans-serif;
            background: var(--bg);
            color: var(--ink);
            line-height: 1.65;
            font-size: 14px;
        }

        .page {
            max-width: 860px;
            margin: 32px auto;
            background: var(--card);
            padding: 3rem 3.4rem;
            box-shadow: 0 2px 16px rgba(0,0,0,0.06);
            border: 1px solid var(--border);
        }

        /* HEADER */
        .header { margin-bottom: 1.8rem; border-bottom: 2px solid var(--ink); padding-bottom: 1rem; }
        .header h1 {
            font-family: 'EB Garamond', serif;
            font-size: 2.4rem;
            font-weight: 600;
            letter-spacing: -0.01em;
            color: var(--ink);
            margin-bottom: 0.3rem;
        }
        .header .tagline {
            font-size: 0.88rem;
            color: var(--muted);
            font-weight: 500;
            margin-bottom: 0.45rem;
        }
        .header .contacts {
            font-size: 0.83rem;
            color: var(--muted);
        }
        .header .contacts a { color: var(--accent); text-decoration: none; }
        .header .contacts a:hover { text-decoration: underline; }

        /* SECTION */
        h2 {
            font-family: 'Inter', sans-serif;
            font-size: 0.72rem;
            font-weight: 700;
            text-transform: uppercase;
            letter-spacing: 0.1em;
            color: var(--accent);
            border-bottom: 1px solid var(--rule);
            padding-bottom: 0.25rem;
            margin: 1.8rem 0 0.85rem;
        }

        /* ENTRY */
        .entry { margin-bottom: 1.3rem; }
        .entry-head { display: flex; justify-content: space-between; align-items: baseline; gap: 1rem; flex-wrap: wrap; }
        .entry-title { font-weight: 700; font-size: 0.95rem; }
        .entry-sub { font-size: 0.85rem; color: var(--muted); }
        .entry-date { font-size: 0.8rem; color: var(--muted); white-space: nowrap; font-style: italic; }

        /* EDUCATION HIGHLIGHT */
        .edu-highlight {
            background: #fffbeb;
            border-left: 3px solid #f59e0b;
            padding: 0.5rem 0.85rem;
            margin: 0.5rem 0;
            font-size: 0.85rem;
            border-radius: 0 4px 4px 0;
        }
        .edu-highlight strong { color: #92400e; }

        /* BULLETS */
        ul.bl { list-style: none; margin-top: 0.4rem; }
        ul.bl li {
            position: relative;
            padding-left: 1.1rem;
            margin-bottom: 0.28rem;
            font-size: 0.88rem;
            color: #1f2937;
        }
        ul.bl li::before { content: '—'; color: var(--muted); position: absolute; left: 0; }

        /* RESEARCH QUESTION CALLOUT */
        .rq {
            background: var(--accent-light);
            border-left: 3px solid var(--accent);
            padding: 0.5rem 0.85rem;
            margin: 0.5rem 0;
            font-size: 0.85rem;
            border-radius: 0 4px 4px 0;
        }
        .rq strong { color: var(--accent); }

        /* PAPER ITEM */
        .paper { margin-bottom: 1.4rem; padding-left: 1rem; border-left: 2px solid var(--rule); }
        .paper-title { font-size: 0.9rem; font-weight: 600; font-style: italic; }
        .paper-meta { font-size: 0.8rem; color: var(--muted); margin-top: 0.1rem; }
        .paper-status {
            display: inline-block;
            font-size: 0.72rem;
            font-weight: 700;
            padding: 0.1rem 0.45rem;
            border-radius: 3px;
            margin-left: 0.4rem;
            background: #fef3c7;
            color: #92400e;
        }
        .paper-status.pr { background: var(--green-bg); color: var(--green); }

        /* ASSUMPTION NOTE */
        .assumption {
            display: inline-block;
            font-size: 0.72rem;
            color: var(--muted);
            font-style: italic;
            margin-left: 0.3rem;
        }

        /* TAGS */
        .tags { margin: 0.4rem 0; }
        .tag {
            display: inline-block;
            background: var(--accent-light);
            color: var(--accent);
            padding: 0.14rem 0.46rem;
            border-radius: 3px;
            font-size: 0.74rem;
            font-weight: 600;
            margin: 0.07rem;
        }

        /* SKILLS */
        .skills-row { display: flex; flex-wrap: wrap; gap: 0.35rem; margin-top: 0.45rem; }
        .st {
            background: #f1f5f9;
            color: var(--ink);
            padding: 0.18rem 0.5rem;
            border-radius: 3px;
            font-size: 0.8rem;
        }

        /* AWARDS TABLE */
        table.aw { width: 100%; font-size: 0.87rem; border-collapse: collapse; }
        table.aw td { padding: 0.28rem 0.5rem; vertical-align: top; }
        table.aw td:first-child { font-weight: 600; white-space: nowrap; padding-right: 1.2rem; }
        table.aw td:last-child { color: var(--muted); font-size: 0.82rem; }

        /* GITHUB LINK */
        .gh { font-size: 0.8rem; color: var(--accent); font-weight: 600; margin-top: 0.3rem; display: inline-block; }
        .gh::before { content: '↗ '; }

        @media (max-width: 640px) {
            .page { padding: 1.5rem; }
            .header h1 { font-size: 1.8rem; }
        }
    </style>
</head>
<body>
<div class="page">

    <!-- ── HEADER ── -->
    <div class="header">
        <h1>Vishwajeet Shashikant Adkine</h1>
        <div class="tagline">AI Researcher &nbsp;·&nbsp; High School Student, India &nbsp;·&nbsp; Independent</div>
        <div class="contacts">
            <a href="mailto:vishwajeetadkine705@gmail.com">vishwajeetadkine705@gmail.com</a>
            &nbsp;·&nbsp; <a href="https://personalprotfolio-vishwajeet-kh8z.vercel.app/">Portfolio</a>
            &nbsp;·&nbsp; <a href="https://vishwajeetadkine.blogspot.com/">Research Blog</a>
            &nbsp;·&nbsp; GitHub: <a href="#">github.com/vishwajeet-adkine</a>
        </div>
    </div>

    <!-- ── RESEARCH STATEMENT ── -->
    <h2>Research Statement</h2>
    <p style="font-size:0.88rem; color:#374151; line-height:1.75;">
        I study the theoretical foundations of deep learning — particularly <strong>optimisation dynamics</strong>, <strong>spectral properties of neural operators</strong>, and <strong>AI safety and reliability</strong>. My current work examines why gradient-based learning converges or fails, how attention mechanisms organise information spectrally, and how interpretable ML pipelines can be hardened against adversarial misuse. My longer-term interest is understanding generalisation through the lens of dynamical systems theory.
    </p>

    <!-- ── EDUCATION ── -->
    <h2>Education</h2>
    <div class="entry">
        <div class="entry-head">
            <div>
                <div class="entry-title">Gokulnath Higher Secondary School &nbsp;·&nbsp; Nanded, India</div>
                <div class="entry-sub">Class 12 (expected 2025) — Mathematics, Physics, Computer Science</div>
            </div>
            <div class="entry-date">2023–2025</div>
        </div>
        <div class="edu-highlight">
            <strong>Note for reviewers:</strong> All research manuscripts and technical implementations listed below were completed independently during secondary school, without institutional affiliation or faculty supervision.
        </div>
        <ul class="bl">
            <li>Class 11 final: <strong>93.17%</strong> (school rank 1) &nbsp;·&nbsp; Class 12 projected: 94–98%</li>
        </ul>
    </div>

    <!-- ── RESEARCH MANUSCRIPTS ── -->
    <h2>Research Manuscripts</h2>
    <p style="font-size:0.8rem; color:var(--muted); margin-bottom:0.8rem; font-style:italic;">All manuscripts are independent works; none have undergone formal peer review. Theoretical claims are analytical derivations under stated assumptions, supported by numerical simulation.</p>

    <div class="paper">
        <div class="paper-title">"Gradient Descent as a Dynamical System: Stability, Phase Transitions, and Convergence" <span class="paper-status">Independent Manuscript · 2025</span></div>
        <div class="paper-meta">Vishwajeet Adkine &nbsp;·&nbsp; <a href="#" style="color:var(--accent);font-size:0.8rem;">PDF ↗</a></div>
        <div class="rq"><strong>Central question:</strong> Under what conditions does gradient descent on a smooth loss behave as a stable fixed-point iteration, and how does learning rate govern qualitative phase-transition behaviour?</div>
        <ul class="bl">
            <li>Framed the gradient descent update as a discrete-time dynamical map; characterised fixed points as stationary points of the loss, independent of step size.</li>
            <li>Derived conditions for local stability in terms of spectral radius of the update operator; identified an analytically optimal learning rate that minimises the condition-number-dependent convergence factor.</li>
            <li>Characterised three qualitative regimes — geometric convergence, critical oscillation, and divergence — via bifurcation analysis; derived sample complexity scaling with condition number.</li>
            <li>Discussed momentum as a mechanism for complex-eigenvalue rotation, achieving improved convergence scaling over vanilla gradient descent.</li>
            <li>Accompanied by open-source interactive visualisation (trajectory explorer, bifurcation diagram, eigenvalue explorer).</li>
        </ul>
        <a href="#" class="gh">github.com/vishwajeet-adkine/gradient-dynamics</a>
    </div>

    <div class="paper">
        <div class="paper-title">"Spectral Properties of Attention Matrices: Eigenvalue Concentration, Singular Value Decay, Norm Scaling, and Expressivity Bounds" <span class="paper-status">Independent Manuscript · 2025</span></div>
        <div class="paper-meta">Vishwajeet Adkine &nbsp;·&nbsp; <a href="#" style="color:var(--accent);font-size:0.8rem;">PDF ↗</a></div>
        <div class="rq"><strong>Central question:</strong> How does the spectral structure of softmax attention matrices evolve during training, and what does this imply for expressivity, stability, and sparsity in transformers?</div>
        <ul class="bl">
            <li>Analysed conditions under which training dynamics drive attention eigenvalues toward dominant real components; leveraged the row-stochastic structure of softmax to bound the full spectrum within the unit disk.</li>
            <li>Examined power-law decay of singular values empirically and analytically; characterised how effective rank collapses in deeper layers — with implications for low-rank approximation methods.</li>
            <li>Derived bounds on Frobenius and spectral norms of attention matrices under softmax normalisation <span class="assumption">(under assumptions stated in the manuscript)</span>; discussed implications for long-context numerical stability.</li>
            <li>Formalised a sparsity–expressivity trade-off via an information-theoretic argument linking effective rank to attention entropy; derived practical guidelines for sparsity thresholds.</li>
            <li>Accompanied by open-source interactive visualisation (eigenvalue evolution, singular decay, norm scaling, sparsity and expressivity explorers).</li>
        </ul>
        <a href="#" class="gh">github.com/vishwajeet-adkine/attention-spectral</a>
    </div>

    <div class="paper">
        <div class="paper-title">"An Interpretable Multi-Signal Scam Detection System Using Machine Learning and Large Language Models" <span class="paper-status pr">Published — Research Blog · Feb 2025</span></div>
        <div class="paper-meta">Vishwajeet Adkine &nbsp;·&nbsp; <a href="https://vishwajeetadkine.blogspot.com/" style="color:var(--accent);font-size:0.8rem;">Read ↗</a></div>
        <div class="rq"><strong>Central question:</strong> Can a lightweight interpretable ML pipeline match LLM-scale accuracy for scam detection while remaining practical for real-time mobile deployment?</div>
        <ul class="bl">
            <li><strong>Data:</strong> ~28,000 URLs (balanced; malicious from URLhaus, benign from Tranco Top 1M); 1,000 text samples synthesised via structured templates.</li>
            <li><strong>Approach:</strong> 16 hand-engineered heuristic features; Logistic Regression with L2 regularisation and Platt calibration (text); Random Forest (URL); ensemble with cost-gated LLM fallback on low-confidence inputs.</li>
            <li><strong>Results:</strong> Scam recall 1.00, scam F1 0.97, ROC-AUC ~0.99, 5-fold CV ~0.97 ± 0.02, URL accuracy 99.96%, inference &lt;5 ms per query.</li>
            <li><strong>Cost design:</strong> LLM invoked only when confidence ∈ [0.4, 0.6] — approximately 90% API call reduction; documented bias analysis and weekly retraining protocol.</li>
        </ul>
    </div>

    <div class="paper">
        <div class="paper-title">Working Notes &amp; Exploratory Research</div>
        <ul class="bl" style="margin-top:0.35rem;">
            <li>Self-verification prompting for hallucination reduction in LLM-based chatbots.</li>
            <li>Prompt injection defence via semantic context sanitisation.</li>
            <li>Improving temporal grounding in LLMs via real-time API injection.</li>
        </ul>
    </div>

    <!-- ── TECHNICAL IMPLEMENTATION ── -->
    <h2>Research Implementation</h2>

    <div class="entry">
        <div class="entry-head">
            <div>
                <div class="entry-title">Character-Level LSTM — Pure NumPy, From Scratch</div>
                <div class="entry-sub">Empirical study in gradient dynamics and numerical optimisation</div>
            </div>
            <div class="entry-date">2024</div>
        </div>
        <div class="rq"><strong>Motivation:</strong> Does implementing BPTT manually in raw NumPy — without any autograd framework — surface gradient failure modes that abstracted libraries conceal?</div>
        <ul class="bl">
            <li>Built complete LSTM forward and backward pass, embedding layer, and linear decoder in NumPy; no PyTorch, TensorFlow, or autograd dependency.</li>
            <li>Manual BPTT with Adam optimiser; gradient clipping and Xavier initialisation to directly observe and study vanishing/exploding gradient behaviour.</li>
            <li>Training loss reduced from 3.258 to 0.076 over 300 epochs; implementation served as an empirical testbed for the dynamical systems manuscript above.</li>
        </ul>
        <a href="#" class="gh">github.com/vishwajeet-adkine/lstm-numpy</a>
    </div>

    <!-- ── APPLIED PROJECTS ── -->
    <h2>Applied Research Systems</h2>

    <div class="entry">
        <div class="entry-head">
            <div>
                <div class="entry-title">Stremini AI — Safety-First AI Pipeline</div>
                <div class="entry-sub">Scam detection · agentic code repair · LLM reliability</div>
            </div>
        </div>
        <ul class="bl">
            <li>Multi-signal scam detection engine (text + URL + image) integrating WHOIS and Safe Browsing APIs; real-world deployment of the paper above.</li>
            <li>Deterministic code-generation agent with autonomous "Read–Fix–Verify" remediation loop — designed around failure-mode analysis rather than capability maximisation.</li>
            <li>Context-aware LLM keyboard with heuristic preprocessing and hallucination mitigation; serverless inference via Cloudflare Workers.</li>
        </ul>
    </div>

    <div class="entry">
        <div class="entry-head">
            <div>
                <div class="entry-title">Scholira — Hallucination-Free Scholarship Retrieval</div>
                <div class="entry-sub">TF-IDF + deterministic rules · deliberate non-generative design</div>
            </div>
        </div>
        <ul class="bl">
            <li>Deliberately chose non-generative retrieval (TF-IDF + rule-based matching) to guarantee factual accuracy in scholarship eligibility decisions — a case study in when not to use LLMs.</li>
        </ul>
    </div>

    <!-- ── HONOURS ── -->
    <h2>Honors &amp; Awards</h2>
    <table class="aw">
        <tr><td>National Rank 14</td><td>SCO Mathematics Olympiad</td></tr>
        <tr><td>Certificate of Excellence + Letter of Recommendation</td><td>IMUN Mathematics Olympiad</td></tr>
        <tr><td>Top International Rank</td><td>School Connect AI Olympiad</td></tr>
    </table>

    <!-- ── CERTIFICATIONS ── -->
    <h2>Certifications</h2>
    <ul class="bl">
        <li><strong>Machine Learning Specialisation</strong> — Stanford University / DeepLearning.AI (Andrew Ng)</li>
        <li><strong>Python for Everybody</strong> — University of Michigan</li>
        <li><strong>Prompt Engineering for Developers</strong> — DeepLearning.AI</li>
    </ul>

    <!-- ── TECHNICAL SKILLS ── -->
    <h2>Technical Skills</h2>
    <div class="skills-row">
        <span class="st">Python</span>
        <span class="st">NumPy</span>
        <span class="st">Scikit-learn</span>
        <span class="st">LSTM (from scratch)</span>
        <span class="st">Feature Engineering</span>
        <span class="st">NLP / TF-IDF</span>
        <span class="st">RAG</span>
        <span class="st">LLM Orchestration</span>
        <span class="st">Agentic Systems</span>
        <span class="st">Prompt Engineering</span>
        <span class="st">Flask / FastAPI</span>
        <span class="st">React / TypeScript</span>
        <span class="st">Dart / Flutter</span>
        <span class="st">Cloudflare Workers</span>
        <span class="st">Firebase</span>
        <span class="st">C++</span>
        <span class="st">Git</span>
    </div>

    <!-- ── REFERENCES ── -->
    <h2>References</h2>
    <p style="font-size:0.87rem; color:var(--muted);">Available upon request.</p>

</div>
</body>
</html>
